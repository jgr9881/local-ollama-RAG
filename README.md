The embeddings and the answer reconstruction are done with a local ollama server running.

You can install and locally use the LLM of your choice : I used nomic-embed-text for the chunk embeddings and Mistral for the answer reconstruction.

This can be setup in the config.py file, or directly on the UI by choosing your models on the left side of the page, and then reloading the page.
